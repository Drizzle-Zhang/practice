# Task1 随机森林算法梳理


## 1. 集成学习的概念

集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，集成学习的一般结构为:先产生一组"个体学习器" (individual learner) ，
再用某种策略将它们结合起来。示意图如下：<br>
![](https://github.com/Drizzle-Zhang/practice/blob/master/ensemble_learning/Supp_Task1/ensemble.png)

## 2. 个体学习器的概念
个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5 决策树算法、BP 神经网络算法等，此时集成中只包含同种类型的个体学习器，
例如"决策树集成"中全是决策树"；神经网络集成"中全是神经网络，这样的集成是"同质"的(homogeneous) 。同质集成中的个体学习器亦称"基学习器" (base learner),
相应的学习算法称为"基学习算法" (base learning algorithm). 集成也可包含不同类型的个体学习器，
例如同时包含决策树和神经网络，这样的集成是"异质"的(heterogenous) 。
异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法;相应的，
个体学习器一般不称为基学习器，常称为"组件学习器" (component learner) 或直接称为个体学习器.

## 3. boosting bagging的概念、异同点
### Boosting
**Boosting** 是一族可将弱学习器提升为强学习器的算法.这族算法的工作机
制类似:先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练
样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，
然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学
习器数目达到事先指定的值T ， 最终将这T 个基学习器进行加权结合.<br>

### Bagging
**Bagging** 是井行式集成学习方法最著名的代表.它直接基于自助来样法(bootstrap sampling).
给定包含m 个样本的数据集，我们先随机取出一个样本放入采样集中，再把该
样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m
次随机采样操作，我们得到含m 个样本的采样集，初始训练集中有的样本在采
样集里多次出现，有的则从未出现.初始训练集中约有63.2%
的样本出现在来样集中.
照这样，我们可采样出T 个含m 个训练样本的采样集，然后基于每个采样
集训练出一个基学习器，再将这些基学习器进行结合.这就是Bagging 的基本
流程.在对预测输出进行结合时， Bagging 通常对分类任务使用简单投票法，对
回归任务使用简单平均法.若分类预测时出现两个类收到同样票数的情形，则
最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最
终胜者.<br>

### Boosting与Bagging的异同点
**样本选择上：** Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是买个样的权重。<br>

**样本权重上：** Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大。<br>

**预测函数上：** Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。<br>

**并行计算：** Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成。<br>


## 4. 理解不同的结合策略(平均法，投票法，学习法)

### 平均法
对数值型输出![](http://latex.codecogs.com/gif.latex?\$$h_{i}(x)\in\mathbb{R}$$)，最常见的结合策略是使用平均法<br>
* 简单平均法<br>
![](http://latex.codecogs.com/gif.latex?\$$H(x)=\frac{1}{T}\sum_{i=1}^{T}h_{i}(x)$$)<br>
* 加权平均法<br>
![](http://latex.codecogs.com/gif.latex?\$$H(x)=\sum_{i=1}^{T}\omega_{i}h_{i}(x)$$)<br>

### 投票法
对分类任务来说，学习器将从类别标记集合中预测出一个标记, 最常见的结合策略是使用投票法(voting).<br>
* 绝对多数投票法<br>
若某标记得票过半数，则预测为该标记;否则拒绝预测.<br>
* 相对多数投票法<br>
预测为得票最多的标记，若同时有多个标记获最高票，则从中随机选取一个.<br>
* 加权投票法<br>
对各个学习器的结果进行加权后，使用相对多数投票法。<br>

### 学习法
当训练数据很多时，一种更为强大的结合策略是使用"学习法"，即通过另一个学习器来进行结合.Stacking是学习法
的典型代表.这里我们把个体学习器称为初级学习器，用于结合的学习器称为次级学习器。<br>
Stacking 先从初始数据集训练出初级学习器，然后"生成"一个新数据集
用于训练次级学习器.在这个新数据集中，初级学习器的输出被当作样例输入
特征，而初始样本的标记仍被当作样例标记.<br>


## 5. 随机森林的思想
随机森林(Random Forest ，简称RF) 是Baggi吨的一个
扩展变体.RF在以决策树为基学习器构建Bagging 集成的基础上，进一步在
决策树的训练过程中引入了随机属性选择.具体来说，传统决策树在选择划分
属性时是在当前结点的属性集合(假定有d 个属性)中选择一个最优属性;而在
RF 中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含k
个属性的子集，然后再从这个子集中选择一个最优属性用于划分. 这里的参数
k 控制了随机性的引入程度: 若令k = d ， 则基决策树的构建与传统决策树相同;
若令k = 1 ， 则是随机选择一个属性用于划分; 一般情况下，推荐值k = log2d.


