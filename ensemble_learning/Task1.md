# Task1 随机森林算法梳理


## 1. 集成学习的概念

集成学习(ensemble learning)通过构建并结合多个学习器来完成学习任务，集成学习的一般结构为:先产生一组"个体学习器" (individual learner) ，
再用某种策略将它们结合起来。示意图如下：<br>
![](https://github.com/Drizzle-Zhang/practice/blob/master/ensemble_learning/Supp_Task1/ensemble.png)

## 2. 个体学习器的概念
个体学习器通常由一个现有的学习算法从训练数据产生，例如C4.5 决策树算法、BP 神经网络算法等，此时集成中只包含同种类型的个体学习器，
例如"决策树集成"中全是决策树"；神经网络集成"中全是神经网络，这样的集成是"同质"的(homogeneous) 。同质集成中的个体学习器亦称"基学习器" (base learner),
相应的学习算法称为"基学习算法" (base learning algorithm). 集成也可包含不同类型的个体学习器，
例如同时包含决策树和神经网络，这样的集成是"异质"的(heterogenous) 。
异质集成中的个体学习器由不同的学习算法生成，这时就不再有基学习算法;相应的，
个体学习器一般不称为基学习器，常称为"组件学习器" (component learner) 或直接称为个体学习器.

## 3. boosting bagging的概念、异同点
### Boosting
**Boosting** 是一族可将弱学习器提升为强学习器的算法.这族算法的工作机
制类似:先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练
样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，
然后基于调整后的样本分布来训练下一个基学习器;如此重复进行，直至基学
习器数目达到事先指定的值T ， 最终将这T 个基学习器进行加权结合.<br>

### Bagging
**Bagging** 是井行式集成学习方法最著名的代表.它直接基于自助来样法(bootstrap sampling).
给定包含m 个样本的数据集，我们先随机取出一个样本放入采样集中，再把该
样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过m
次随机采样操作，我们得到含m 个样本的采样集，初始训练集中有的样本在采
样集里多次出现，有的则从未出现.初始训练集中约有63.2%
的样本出现在来样集中.
照这样，我们可采样出T 个含m 个训练样本的采样集，然后基于每个采样
集训练出一个基学习器，再将这些基学习器进行结合.这就是Bagging 的基本
流程.在对预测输出进行结合时， Bagging 通常对分类任务使用简单投票法，对
回归任务使用简单平均法.若分类预测时出现两个类收到同样票数的情形，则
最简单的做法是随机选择一个，也可进一步考察学习器投票的置信度来确定最
终胜者.<br>

### Boosting与Bagging的异同点
**样本选择上：** Bagging采取Bootstraping的是随机有放回的取样，Boosting的每一轮训练的样本是固定的，改变的是买个样的权重。<br>

**样本权重上：** Bagging采取的是均匀取样，且每个样本的权重相同，Boosting根据错误率调整样本权重，错误率越大的样本权重会变大。<br>

**预测函数上：** Bagging所以的预测函数权值相同，Boosting中误差越小的预测函数其权值越大。<br>

**并行计算：** Bagging 的各个预测函数可以并行生成;Boosting的各个预测函数必须按照顺序迭代生成。<br>





