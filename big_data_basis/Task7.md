# 【Task7】实践

## 1. 计算每个content的CTR。

下载完成的数据格式如下：
```
uid     content_list    content_id
0       164423,430922,112513,485726,488385,340139,489273,391258 112513
1       635374,409237,586823,305055,519191,772121,788428,754213 305055,586823,305055,305055
2       57518,70020,828660,9511,477360,821209,178443,973485     178443,70020,178443,9511
3       542973,871389,914465,513667,536708,646545,90801,994236  536708
4       530817,401690,813927,107595,472415,375159,11354,281431  530817,375159
5       282200,402105,913036,389736,392579,166522,14420,314787  402105,282200,166522
6       568328,381531,873759,157884,812936,112027,602916,714218 381531,602916,568328,568328,112027,112027
```
计算目标是在每个uid中，计算content_list个数与content_id个数的商<br><br>

启动pyspark，运行下面的代码：
```Python
# 设置数据的路径
contentData = sc.textFile("file:///local/zy/download/content_list_id.txt")

# 计算每个uid的CTR
ctr = contentData.map(lambda x:x.split('\t')).map(lambda line:(line[0], len(line[1].split(','))/len(line[2].split(','))))

# 计算结果如下
>>> ctr.take(10)
[('uid', 1.0), ('0', 8.0), ('1', 2.0), ('2', 2.0), ('3', 8.0), ('4', 4.0), ('5', 2.6666666666666665), ('6', 1.3333333333333333), ('7', 2.0), ('8', 2.0)]

```

## 2. 【选做】 使用Spark实现ALS矩阵分解算法


## 3. 使用Spark分析Amazon DataSet(实现 Spark LR、Spark TFIDF)














