# æ³¨æ„åŠ›æœºåˆ¶

åœ¨â€œç¼–ç å™¨â€”è§£ç å™¨ï¼ˆseq2seqï¼‰â€â¼€èŠ‚â¾¥ï¼Œè§£ç å™¨åœ¨å„ä¸ªæ—¶é—´æ­¥ä¾èµ–ç›¸åŒçš„èƒŒæ™¯å˜é‡ï¼ˆcontext vectorï¼‰æ¥è·å–è¾“â¼Šåºåˆ—ä¿¡æ¯ã€‚å½“ç¼–ç å™¨ä¸ºå¾ªç¯ç¥ç»â½¹ç»œæ—¶ï¼ŒèƒŒæ™¯å˜é‡æ¥â¾ƒå®ƒæœ€ç»ˆæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚å°†æºåºåˆ—è¾“å…¥ä¿¡æ¯ä»¥å¾ªç¯å•ä½çŠ¶æ€ç¼–ç ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™è§£ç å™¨ä»¥ç”Ÿæˆç›®æ ‡åºåˆ—ã€‚ç„¶è€Œè¿™ç§ç»“æ„å­˜åœ¨ç€é—®é¢˜ï¼Œå°¤å…¶æ˜¯**RNNæœºåˆ¶å®é™…ä¸­å­˜åœ¨é•¿ç¨‹æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜**ï¼Œå¯¹äºè¾ƒé•¿çš„å¥å­ï¼Œæˆ‘ä»¬å¾ˆéš¾å¯„å¸Œæœ›äºå°†è¾“å…¥çš„åºåˆ—è½¬åŒ–ä¸ºå®šé•¿çš„å‘é‡è€Œä¿å­˜æ‰€æœ‰çš„æœ‰æ•ˆä¿¡æ¯ï¼Œæ‰€ä»¥éšç€æ‰€éœ€ç¿»è¯‘å¥å­çš„é•¿åº¦çš„å¢åŠ ï¼Œè¿™ç§ç»“æ„çš„æ•ˆæœä¼šæ˜¾è‘—ä¸‹é™ã€‚

ä¸æ­¤åŒæ—¶ï¼Œè§£ç çš„ç›®æ ‡è¯è¯­å¯èƒ½åªä¸åŸè¾“å…¥çš„éƒ¨åˆ†è¯è¯­æœ‰å…³ï¼Œè€Œå¹¶ä¸æ˜¯ä¸æ‰€æœ‰çš„è¾“å…¥æœ‰å…³ã€‚ä¾‹å¦‚ï¼Œå½“æŠŠâ€œHello worldâ€ç¿»è¯‘æˆâ€œBonjour le mondeâ€æ—¶ï¼Œâ€œHelloâ€æ˜ å°„æˆâ€œBonjourâ€ï¼Œâ€œworldâ€æ˜ å°„æˆâ€œmondeâ€ã€‚åœ¨seq2seqæ¨¡å‹ä¸­ï¼Œè§£ç å™¨åªèƒ½éšå¼åœ°ä»ç¼–ç å™¨çš„æœ€ç»ˆçŠ¶æ€ä¸­é€‰æ‹©ç›¸åº”çš„ä¿¡æ¯ã€‚ç„¶è€Œï¼Œæ³¨æ„åŠ›æœºåˆ¶å¯ä»¥å°†è¿™ç§é€‰æ‹©è¿‡ç¨‹æ˜¾å¼åœ°å»ºæ¨¡ã€‚

![Image Name](https://cdn.kesci.com/upload/image/q5km4dwgf9.PNG?imageView2/0/w/960/h/960)

## 1 æ³¨æ„åŠ›æœºåˆ¶æ¡†æ¶

Attention æ˜¯ä¸€ç§é€šç”¨çš„å¸¦æƒæ± åŒ–æ–¹æ³•ï¼Œè¾“å…¥ç”±ä¸¤éƒ¨åˆ†æ„æˆï¼šè¯¢é—®ï¼ˆqueryï¼‰å’Œé”®å€¼å¯¹ï¼ˆkey-value pairsï¼‰ã€‚$ğ¤_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘˜}, ğ¯_ğ‘–âˆˆâ„^{ğ‘‘_ğ‘£}$. Query  $ğªâˆˆâ„^{ğ‘‘_ğ‘}$ , attention layerå¾—åˆ°è¾“å‡ºä¸valueçš„ç»´åº¦ä¸€è‡´ $ğ¨âˆˆâ„^{ğ‘‘_ğ‘£}$. å¯¹äºä¸€ä¸ªqueryæ¥è¯´ï¼Œattention layer ä¼šä¸æ¯ä¸€ä¸ªkeyè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°å¹¶è¿›è¡Œæƒé‡çš„å½’ä¸€åŒ–ï¼Œè¾“å‡ºçš„å‘é‡$o$åˆ™æ˜¯valueçš„åŠ æƒæ±‚å’Œï¼Œè€Œæ¯ä¸ªkeyè®¡ç®—çš„æƒé‡ä¸valueä¸€ä¸€å¯¹åº”ã€‚

ä¸ºäº†è®¡ç®—è¾“å‡ºï¼Œæˆ‘ä»¬é¦–å…ˆå‡è®¾æœ‰ä¸€ä¸ªå‡½æ•°$\alpha$ ç”¨äºè®¡ç®—queryå’Œkeyçš„ç›¸ä¼¼æ€§ï¼Œç„¶åå¯ä»¥è®¡ç®—æ‰€æœ‰çš„ attention scores $a_1, \ldots, a_n$ by


$$
a_i = \alpha(\mathbf q, \mathbf k_i).
$$


æˆ‘ä»¬ä½¿ç”¨ softmaxå‡½æ•° è·å¾—æ³¨æ„åŠ›æƒé‡ï¼š


$$
b_1, \ldots, b_n = \textrm{softmax}(a_1, \ldots, a_n).
$$


æœ€ç»ˆçš„è¾“å‡ºå°±æ˜¯valueçš„åŠ æƒæ±‚å’Œï¼š


$$
\mathbf o = \sum_{i=1}^n b_i \mathbf v_i.
$$


![Image Name](https://cdn.kesci.com/upload/image/q5km4ooyu2.PNG?imageView2/0/w/960/h/960)

ä¸åŒçš„attetion layerçš„åŒºåˆ«åœ¨äºscoreå‡½æ•°çš„é€‰æ‹©ï¼Œåœ¨æœ¬èŠ‚çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸¤ä¸ªå¸¸ç”¨çš„æ³¨æ„å±‚ Dot-product Attention å’Œ Multilayer Perceptron Attentionï¼›éšåæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªå¼•å…¥attentionçš„seq2seqæ¨¡å‹å¹¶åœ¨è‹±æ³•ç¿»è¯‘è¯­æ–™ä¸Šè¿›è¡Œè®­ç»ƒä¸æµ‹è¯•ã€‚

### Softmaxå±è”½

åœ¨æ·±å…¥ç ”ç©¶å®ç°ä¹‹å‰ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»softmaxæ“ä½œç¬¦çš„ä¸€ä¸ªå±è”½æ“ä½œã€‚

å› ä¸ºä¸ºäº†ç»Ÿä¸€å¥å­é•¿åº¦ï¼Œä¹‹å‰åŠ å…¥äº†paddingç¬¦å·ã€‚ä½†æ˜¯å¯¹äºAMï¼Œä¸éœ€è¦è€ƒè™‘paddingï¼Œæ‰€ä»¥å°†å…¶å˜ä¸ºè´Ÿæ— ç©·

```python
import math
import torch
import torch.nn as nn


# Softmaxå±è”½
def SequenceMask(X, X_len, value=-1e6):
    maxlen = X.size(1)
    # print(X.size(),torch.arange((maxlen),dtype=torch.float)[None, :],'\n',X_len[:, None] )
    mask = torch.arange((maxlen), dtype=torch.float)[None, :] >= X_len[:, None]
    # print(mask)
    X[mask] = value
    return X


def masked_softmax(X, valid_length):
    # X: 3-D tensor, batch_size*seq_len*dim
    # valid_length: 1-D or 2-D tensor
    softmax = nn.Softmax(dim=-1)
    if valid_length is None:
        return softmax(X)
    else:
        shape = X.shape
        if valid_length.dim() == 1:
            try:
                valid_length = torch.FloatTensor(
                    valid_length.numpy().repeat(shape[1], axis=0))
                # [2,3] -> [2,2,3,3] å¯¹äºæœ€å¤§é•¿åº¦ä¸º2çš„batchæ¥è¯´
                # ä½œç”¨æ˜¯å’Œå˜æ¢å½¢çŠ¶åçš„Xç»´æ•°ä¸€è‡´
            except:
                valid_length = torch.FloatTensor(
                    valid_length.cpu().numpy().repeat(shape[1],
                                                      axis=0))  # [2,2,3,3]
        else:
            valid_length = valid_length.reshape((-1,))
        # fill masked elements with a large negative, whose exp is 0
        X = SequenceMask(X.reshape((-1, shape[-1])), valid_length)

        return softmax(X).reshape(shape)


masked_softmax(torch.rand((2, 2, 4), dtype=torch.float),
               torch.FloatTensor([2, 3]))
```

```
tensor([[[0.4566, 0.5434, 0.0000, 0.0000],
         [0.4910, 0.5090, 0.0000, 0.0000]],

        [[0.2537, 0.3907, 0.3556, 0.0000],
         [0.3347, 0.2245, 0.4408, 0.0000]]])
```

**è¶…å‡º2ç»´çŸ©é˜µçš„ä¹˜æ³•** 

$X$ å’Œ $Y$ æ˜¯ç»´åº¦åˆ†åˆ«ä¸º$(b,n,m)$ å’Œ$(b, m, k)$çš„å¼ é‡ï¼Œè¿›è¡Œ $b$ æ¬¡äºŒç»´çŸ©é˜µä¹˜æ³•åå¾—åˆ° $Z$, ç»´åº¦ä¸º $(b, n, k)$ã€‚


$$
Z[i,:,:] = dot(X[i,:,:], Y[i,:,:])\qquad for\ i= 1,â€¦,n\ .
$$

> é«˜ç»´å¼ é‡çš„çŸ©é˜µä¹˜æ³•å¯ç”¨äºå¹¶è¡Œè®¡ç®—å¤šä¸ªä½ç½®çš„æ³¨æ„åŠ›åˆ†æ•°ã€‚

```python
torch.bmm(torch.ones((2,1,3), dtype = torch.float), torch.ones((2,3,2), dtype = torch.float))
```

```
tensor([[[3., 3.]],

        [[3., 3.]]])
```

## ç‚¹ç§¯æ³¨æ„åŠ›

The dot product å‡è®¾queryå’Œkeysæœ‰ç›¸åŒçš„ç»´åº¦, å³ $\forall i, ğª,ğ¤_ğ‘– âˆˆ â„_ğ‘‘ $. é€šè¿‡è®¡ç®—queryå’Œkeyè½¬ç½®çš„ä¹˜ç§¯æ¥è®¡ç®—attention score,é€šå¸¸è¿˜ä¼šé™¤å» $\sqrt{d}$ å‡å°‘è®¡ç®—å‡ºæ¥çš„scoreå¯¹ç»´åº¦ğ‘‘çš„ä¾èµ–æ€§ï¼Œå¦‚ä¸‹


$$
ğ›¼(ğª,ğ¤)=âŸ¨ğª,ğ¤âŸ©/ \sqrt{d} 
$$

å‡è®¾ $ ğâˆˆâ„^{ğ‘šÃ—ğ‘‘}$ æœ‰ $m$ ä¸ªqueryï¼Œ$ğŠâˆˆâ„^{ğ‘›Ã—ğ‘‘}$ æœ‰ $n$ ä¸ªkeys. æˆ‘ä»¬å¯ä»¥é€šè¿‡çŸ©é˜µè¿ç®—çš„æ–¹å¼è®¡ç®—æ‰€æœ‰ $mn$ ä¸ªscoreï¼š


$$
ğ›¼(ğ,ğŠ)=ğğŠ^ğ‘‡/\sqrt{d}
$$

ç°åœ¨è®©æˆ‘ä»¬å®ç°è¿™ä¸ªå±‚ï¼Œå®ƒæ”¯æŒä¸€æ‰¹æŸ¥è¯¢å’Œé”®å€¼å¯¹ã€‚æ­¤å¤–ï¼Œå®ƒæ”¯æŒä½œä¸ºæ­£åˆ™åŒ–éšæœºåˆ é™¤ä¸€äº›æ³¨æ„åŠ›æƒé‡.

```python
class DotProductAttention(nn.Module):
    def __init__(self, dropout, **kwargs):
        super(DotProductAttention, self).__init__(**kwargs)
        self.dropout = nn.Dropout(dropout)

    # query: (batch_size, #queries, d)
    # key: (batch_size, #kv_pairs, d)
    # value: (batch_size, #kv_pairs, dim_v)
    # valid_length: either (batch_size, ) or (batch_size, xx)
    def forward(self, query, key, value, valid_length=None):
        d = query.shape[-1]
        # set transpose_b=True to swap the last two dimensions of key

        scores = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(d)
        attention_weights = self.dropout(masked_softmax(scores, valid_length))
        print("attention_weight\n", attention_weights)
        return torch.bmm(attention_weights, value)

```

#### æµ‹è¯•

ç°åœ¨æˆ‘ä»¬åˆ›å»ºäº†ä¸¤ä¸ªæ‰¹ï¼Œæ¯ä¸ªæ‰¹æœ‰ä¸€ä¸ªqueryå’Œ10ä¸ªkey-valueså¯¹ã€‚æˆ‘ä»¬é€šè¿‡valid_lengthæŒ‡å®šï¼Œå¯¹äºç¬¬ä¸€æ‰¹ï¼Œæˆ‘ä»¬åªå…³æ³¨å‰2ä¸ªé”®-å€¼å¯¹ï¼Œè€Œå¯¹äºç¬¬äºŒæ‰¹ï¼Œæˆ‘ä»¬å°†æ£€æŸ¥å‰6ä¸ªé”®-å€¼å¯¹ã€‚å› æ­¤ï¼Œå°½ç®¡è¿™ä¸¤ä¸ªæ‰¹å¤„ç†å…·æœ‰ç›¸åŒçš„æŸ¥è¯¢å’Œé”®å€¼å¯¹ï¼Œä½†æˆ‘ä»¬è·å¾—çš„è¾“å‡ºæ˜¯ä¸åŒçš„ã€‚

```python
# example
atten = DotProductAttention(dropout=0)

keys = torch.ones((2,10,2),dtype=torch.float)
values = torch.arange((40), dtype=torch.float).view(1,10,4).repeat(2,1,1)
atten(torch.ones((2,1,2),dtype=torch.float), keys, values, torch.FloatTensor([2, 6]))

```

```
tensor([[[0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,
          0.0000, 0.0000]],

        [[0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000,
          0.0000, 0.0000]]])
tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],

        [[10.0000, 11.0000, 12.0000, 13.0000]]])
```

## å¤šå±‚æ„ŸçŸ¥æœºæ³¨æ„åŠ›

åœ¨å¤šå±‚æ„ŸçŸ¥å™¨ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆå°† query and keys æŠ•å½±åˆ°  $â„^â„$ .ä¸ºäº†æ›´å…·ä½“ï¼Œæˆ‘ä»¬å°†å¯ä»¥å­¦ä¹ çš„å‚æ•°åšå¦‚ä¸‹æ˜ å°„ 
$ğ–_ğ‘˜âˆˆâ„^{â„Ã—ğ‘‘_ğ‘˜}$ ,  $ğ–_ğ‘âˆˆâ„^{â„Ã—ğ‘‘_ğ‘}$ , and  $ğ¯âˆˆâ„^h$ . å°†scoreå‡½æ•°å®šä¹‰
$$
ğ›¼(ğ¤,ğª)=ğ¯^ğ‘‡tanh(ğ–_ğ‘˜ğ¤+ğ–_ğ‘ğª)
$$

ç„¶åå°†key å’Œ value åœ¨ç‰¹å¾çš„ç»´åº¦ä¸Šåˆå¹¶ï¼ˆconcatenateï¼‰ï¼Œç„¶åé€è‡³ a single hidden layer perceptron è¿™å±‚ä¸­ hidden layer ä¸º  â„  and è¾“å‡ºçš„sizeä¸º 1 .éšå±‚æ¿€æ´»å‡½æ•°ä¸ºtanhï¼Œæ— åç½®.

```python
class MLPAttention(nn.Module):
    def __init__(self, units,ipt_dim,dropout, **kwargs):
        super(MLPAttention, self).__init__(**kwargs)
        # Use flatten=True to keep query's and key's 3-D shapes.
        self.W_k = nn.Linear(ipt_dim, units, bias=False)
        self.W_q = nn.Linear(ipt_dim, units, bias=False)
        self.v = nn.Linear(units, 1, bias=False)
        self.dropout = nn.Dropout(dropout)

    def forward(self, query, key, value, valid_length):
        query, key = self.W_k(query), self.W_q(key)
        #print("size",query.size(),key.size())
        # expand query to (batch_size, #querys, 1, units), and key to
        # (batch_size, 1, #kv_pairs, units). Then plus them with broadcast.
        features = query.unsqueeze(2) + key.unsqueeze(1)
        #print("features:",features.size())  #--------------å¼€å¯
        scores = self.v(features).squeeze(-1)
        attention_weights = self.dropout(masked_softmax(scores, valid_length))
        return torch.bmm(attention_weights, value)

```

#### æµ‹è¯•

å°½ç®¡MLPAttentionåŒ…å«ä¸€ä¸ªé¢å¤–çš„MLPæ¨¡å‹ï¼Œä½†å¦‚æœç»™å®šç›¸åŒçš„è¾“å…¥å’Œç›¸åŒçš„é”®ï¼Œæˆ‘ä»¬å°†è·å¾—ä¸DotProductAttentionç›¸åŒçš„è¾“å‡º

```
atten = MLPAttention(ipt_dim=2, units=8, dropout=0)
atten(torch.ones((2, 1, 2), dtype=torch.float), keys, values,
      torch.FloatTensor([2, 6]))

```

```
tensor([[[ 2.0000,  3.0000,  4.0000,  5.0000]],

        [[10.0000, 11.0000, 12.0000, 13.0000]]], grad_fn=<BmmBackward>)
```

> åœ¨Dot-product Attentionä¸­ï¼Œkeyä¸queryç»´åº¦éœ€è¦ä¸€è‡´ï¼Œåœ¨MLP Attentionä¸­åˆ™ä¸éœ€è¦ã€‚

## 2 å¯¹æ³¨æ„åŠ›æœºåˆ¶çš„ç†è§£

### Seq2Seqæ¨¡å‹

æˆ‘ä»¬çŸ¥é“Seq2Seqæ¨¡å‹çš„ç»“æ„æ˜¯åŸºäºç¼–ç å™¨-è§£ç å™¨ï¼Œå¯ä»¥è§£å†³è¾“å…¥å’Œè¾“å‡ºåºåˆ—ä¸ç­‰é•¿çš„é—®é¢˜ï¼Œä¾‹å¦‚æœºå™¨ç¿»è¯‘é—®é¢˜ã€‚ç¼–ç å™¨å’Œè§£ç å™¨æœ¬è´¨ä¸Šæ˜¯ä¸¤ä¸ªRNNï¼Œå…¶ä¸­ç¼–ç å™¨å¯¹è¾“å…¥åºåˆ—è¿›è¡Œåˆ†æç¼–ç æˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡(Context vector)ï¼Œè§£ç å™¨åˆ©ç”¨è¿™ä¸ªç¼–ç å™¨ç”Ÿæˆçš„å‘é‡æ ¹æ®å…·ä½“ä»»åŠ¡æ¥è¿›è¡Œè§£ç ï¼Œå¾—åˆ°ä¸€ä¸ªæ–°çš„åºåˆ—ã€‚

#### ç¼–ç å™¨

å¦‚ä¸‹å›¾æ‰€ç¤ºä¸ºä¸€ä¸ªç¼–ç å™¨çš„ç»“æ„ï¼Œå°±æ˜¯å°†è¾“å…¥åºåˆ—x1x1 x_1 è‡³x4x4 x_4 ä¾æ¬¡è¾“å…¥åˆ°ç¼–ç å™¨ä¸­å¾—åˆ°äº†h1h1 h_1 è‡³h4h4 h_4 çš„éšå«çŠ¶æ€ï¼Œè€Œæœ€ç»ˆçš„ä¸Šä¸‹æ–‡å‘é‡cc c ,å¯ä»¥æ˜¯ç¼–ç å™¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œä¹Ÿå¯ä»¥æ˜¯ç¼–ç å™¨æ¯ä¸ªæ—¶é—´æ­¥å¾—åˆ°çš„éšè—çŠ¶æ€è¿›è¡Œä¸€ä¸ªå‡½æ•°æ˜ å°„(å°±æ˜¯ä½¿ç”¨æŸä¸ªåº¦é‡å‡½æ•°å»è¡¨ç¤ºåŸå§‹åºåˆ—çš„ä¿¡æ¯)ï¼Œè¿™ä¸ªä¸Šä¸‹æ–‡å‘é‡åé¢ä¼šå†è§£ç å™¨ç”Ÿæˆåºåˆ—ä¸­ã€‚

![preview](https://pic2.zhimg.com/v2-03aaa7754bb9992858a05bb9668631a9_r.jpg) 

#### è§£ç å™¨

ä¸‹å›¾æ˜¯ä¸¤ç§æ¯”è¾ƒå¸¸è§çš„Seq2Seqæ¨¡å‹çš„ç»“æ„ï¼Œä¸¤ä¸ªå›¾çš„å·¦åŠéƒ¨åˆ†éƒ½æ˜¯ä¸Šé¢æ‰€è¯´çš„ç¼–ç å™¨éƒ¨åˆ†ï¼Œè€Œå³åŠéƒ¨åˆ†å°±æ˜¯è§£ç å™¨éƒ¨åˆ†äº†ã€‚å¦‚ä¸‹é¢ç¬¬ä¸€å¼ å›¾æ‰€ç¤ºï¼Œå…¶ç›´æ¥å°†ç¼–ç å™¨çš„è¾“å‡ºä½œä¸ºè§£ç å™¨çš„åˆå§‹éšè—çŠ¶æ€ï¼Œç„¶åç›´æ¥è¿›è¡Œè§£ç ã€‚ç¬¬äºŒå¼ å›¾æ˜¯ç›´æ¥å°†ç¼–ç å™¨å¾—åˆ°çš„ä¸Šä¸‹æ–‡å‘é‡è¾“å…¥åˆ°è§£ç å™¨çš„æ¯ä¸ªæ—¶é—´æ­¥ä¸­ï¼Œå¹¶ä¸”æ¯ä¸ªæ—¶é—´æ­¥çš„ä¸Šä¸‹æ–‡å‘é‡æ˜¯ç›¸åŒï¼Œæ¢å¥è¯è¯´å°±æ˜¯è§£ç å™¨æ¯ä¸ªæ—¶é—´æ­¥éƒ½ä½¿ç”¨äº†ç›¸åŒçš„ä¸Šä¸‹æ–‡å‘é‡ã€‚è¿™ä¸¤ç§æƒ…å†µå¯èƒ½å¸¦æ¥çš„é—®é¢˜æ˜¯ï¼Œå½“éœ€è¦ç¼–ç çš„å¥å­å¤ªé•¿çš„æ—¶å€™ï¼Œç”±äºä¸Šä¸‹æ–‡å‘é‡èƒ½å¤Ÿå­˜å‚¨ä¿¡æ¯çš„å®¹é‡æ˜¯æœ‰é™çš„ï¼Œæ‰€ä»¥å¯èƒ½ä¼šå¯¼è‡´ï¼Œä¿¡æ¯çš„ä¸¢å¤±ï¼Œæ­¤å¤–ï¼Œè§£ç å™¨æ¯ä¸ªæ—¶é—´æ­¥çš„ä¸Šä¸‹æ–‡å‘é‡éƒ½æ˜¯ä¸€ä¸ªç›¸åŒçš„å¯¹è¾“å…¥åºåˆ—çš„è¡¨å¾ï¼Œå¯¹äºä¸Šé¢ä¸¤ç§é—®é¢˜ï¼ŒåŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„Seq2Seqæ¨¡å‹ç»™äº†å¾ˆå¥½çš„è§£å†³åŠæ³•ã€‚

![preview](https://pic4.zhimg.com/v2-77e8a977fc3d43bec8b05633dc52ff9f_r.jpg) 

![preview](https://pic4.zhimg.com/v2-e0fbb46d897400a384873fc100c442db_r.jpg) 

### Attentionæœºåˆ¶çš„Seq2Seq

åŸºäºAttentionçš„Seq2Seqæ¨¡å‹æœ¬è´¨ä¸Šå°±æ˜¯åœ¨ä¸Šè¿°çš„å›¾ä¸‰ä¸­çš„è§£ç å™¨éƒ¨åˆ†è¿›è¡Œäº†æ”¹è¿›ï¼Œåœ¨è§£ç å™¨çš„æ¯ä¸ªæ—¶é—´æ­¥ä¸Šä½¿ç”¨ä¸åŒçš„ä¸Šä¸‹æ–‡å‘é‡c å¦‚ä¸‹å›¾æ‰€ç¤ºçš„$c_1,c_2,c_3$ ï¼Œä½†æ˜¯å¯¹äºè§£ç å™¨çš„åˆå§‹åŒ–ä¸€èˆ¬è¿˜æ˜¯ä¼šä½¿ç”¨ç¼–ç å™¨æœ€åæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå³å›¾ä¸­çš„$h'_0=c $(æ­¤å¤„çš„cè¡¨ç¤ºçš„æ˜¯ç¼–ç å™¨æœ€åæ—¶é—´æ­¥çš„éšè—çŠ¶æ€)ï¼Œå¦‚ä½•å¾—åˆ°è§£ç å™¨ä¸åŒæ—¶é—´æ­¥ä¸åŒçš„ä¸Šä¸‹æ–‡å‘é‡å°±æ˜¯Attentionè¦åšçš„äº‹æƒ…äº†ã€‚

![preview](https://pic2.zhimg.com/v2-8da16d429d33b0f2705e47af98e66579_r.jpg) 

Attentionæœºåˆ¶ç”Ÿæˆçš„ä¸Šä¸‹æ–‡å‘é‡å¯ä»¥è‡ªåŠ¨çš„å»é€‰å–ä¸å½“å‰æ—¶é—´æ­¥è¾“å‡ºæœ€æœ‰ç”¨çš„ä¿¡æ¯ï¼Œç”¨æœ‰é™çš„ä¸Šä¸‹æ–‡å‘é‡çš„å®¹é‡å»è¡¨ç¤ºå½“å‰æ—¶é—´æ­¥å¯¹è¾“å…¥ä¿¡æ¯æœ€å…³æ³¨çš„é‚£éƒ¨åˆ†ä¿¡æ¯ï¼Œ**æœ€ç®€å•çš„åšæ³•å°±æ˜¯å¯¹ç¼–ç å™¨è¾“å‡ºçš„æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€è¿›è¡Œä¸€ä¸ªåŠ æƒå¹³å‡ï¼Œä¸åŒçš„æƒå€¼æ‰€å¯¹åº”çš„éšå«çŠ¶æ€å°±æ˜¯å¯¹ä¸åŒæ—¶é—´æ­¥çš„è¾“å…¥ä¿¡æ¯å…³çš„æ³¨ç¨‹åº¦**ï¼Œä¸‹é¢çš„ç‚¹ç§¯æ¨¡å‹ç¤ºæ„å›¾å¯ä»¥å½¢è±¡çš„è¡¨ç¤ºè¯¥è¿‡ç¨‹ã€‚å›¾ä¸­çš„a è¡¨ç¤ºæ˜¯ç¼–ç å™¨ä¸åŒæ—¶é—´æ­¥å¯¹åº”çš„æƒå€¼ï¼Œè€Œå…¶æƒå€¼åˆå†³å®šäºç¼–ç å™¨è¯¥æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ä»¥åŠè§£ç å™¨ä¸Šä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå› æ­¤æ³¨æ„åŠ›å±‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–æ˜¯ç”±éšè—çŠ¶æ€å†³å®šçš„ï¼Œå³å†³å®šéšè—çŠ¶æ€çš„å‚æ•°çš„ä¼˜åŒ–ä¹ŸåŒ…å«äº†æ³¨æ„åŠ›å±‚çš„ä¿¡æ¯ã€‚ä¸‹é¢ç»™å‡ºä¸€ä¸ªç®€å•çš„è§£é‡Šï¼šè®¾è§£ç å™¨å½“å‰éšè—çŠ¶æ€ä¸º$s_{t'} $

åˆ™æ— æ³¨æ„åŠ›çš„è§£ç å™¨å½“å‰çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºï¼š$s_{t'} = g(y_{t'-1}, c, s_{t'-1}) $
åŸºäºæ³¨æ„åŠ›çš„è§£ç å™¨å½“å‰çš„éšè—çŠ¶æ€è¡¨ç¤ºä¸ºï¼š$s_{t'} = g(y_{t'-1}, c_{t'}, s_{t'-1}) $

å…¶ä¸­ï¼š

$y_{tâ€²âˆ’1}$::  è§£ç å™¨ä¸Šä¸€æ—¶é—´æ­¥çš„è¾“å‡º
c:  ç¼–ç å™¨æœ€åæ—¶é—´æ­¥(æˆ–è€…ä¹‹å‰æ‰€æœ‰æ—¶é—´æ­¥éšè—çŠ¶æ€çš„æŸç§æ˜ å°„)çš„éšè—çŠ¶æ€
$c_{t'}$:  è§£ç å™¨åœ¨t'æ—¶é—´æ­¥é€šè¿‡æ³¨æ„åŠ›æœºåˆ¶è·å¾—çš„çš„ä¸Šä¸‹æ–‡å‘é‡
$s_{t'-1}$: è§£ç å™¨çš„è§£ç å™¨çš„ è§£ç å™¨çš„ $t'-1$æ—¶é—´æ­¥çš„éšè—çŠ¶æ€

ä¸‹é¢ä¸¤ä¸ªå›¾æ˜¯èƒŒæ™¯å˜é‡$c_t'$ çš„ç”Ÿæˆè¿‡ç¨‹ï¼Œæœ€åå°±å‰©ä¸‹å¦‚ä½•è®¡ç®—$a_{ij}$ çš„å€¼äº†ã€‚è¿™é‡Œçš„aå…¶å®$a_{ij}$ æ˜¯æ³¨æ„åŠ›æ‰“åˆ†å‡½æ•°çš„è¾“å‡ºï¼Œè·Ÿä¸‰éƒ¨åˆ†ä¸œè¥¿æœ‰å…³ç³»ï¼Œåˆ†åˆ«æ˜¯æŸ¥è¯¢é¡¹q(quary)q(quary) q(quary) ï¼šè§£ç å™¨ä¸Šä¸€æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ $s_{t'-1}$ ï¼Œé”®é¡¹k(key)å’Œå€¼é¡¹v(value)éƒ½æ˜¯ç¼–ç å™¨çš„éšå«çŠ¶æ€$(h_1, h_2, h_3 )$ã€‚å¸¸è§çš„æ³¨æ„åŠ›æ‰“åˆ†å‡½æ•°æœ‰ï¼š

`åŠ æ€§æ¨¡å‹`ï¼š$s_{(x_i,q)}= v^Ttanh(Wx_i+Uq) $
`ç‚¹ç§¯æ¨¡å‹`ï¼š $s_{(x_i,q)}= x_i^Tq $
`åŒçº¿æ€§æ¨¡å‹`ï¼š$s_{(x_i,q)}= x_i^TW$q$

![preview](https://pic.imgdb.cn/item/5e4c98c548b86553ee7bde57.jpg) 

`ç‚¹ç§¯æ¨¡å‹`å¯è§†åŒ–å¦‚ä¸‹ï¼š

![preview](https://pic1.zhimg.com/v2-d266bf48a1d77e7e4db607978574c9fc_r.jpg) 

æœ€ååŸºäºæ³¨æ„åŠ›çš„Seq2Seqæ¨¡å‹å¯ä»¥ç”¨ä¸‹å›¾è¿›è¡Œè¡¨ç¤ºï¼š

![Image Name](https://pic.imgdb.cn/item/5e4c991148b86553ee7be9be.png)

å‚è€ƒï¼š

[å®Œå…¨å›¾è§£RNNã€RNNå˜ä½“ã€Seq2Seqã€Attentionæœºåˆ¶](https://zhuanlan.zhihu.com/p/28054589)

[Seq2seqæ¨¡å‹åŠæ³¨æ„åŠ›æœºåˆ¶](http://www.ryluo.cn/2020/02/17/Seq2seqæ¨¡å‹åŠæ³¨æ„åŠ›æœºåˆ¶/)

[å›¾è§£ç¥ç»æœºå™¨ç¿»è¯‘ä¸­çš„æ³¨æ„åŠ›æœºåˆ¶](https://zhuanlan.zhihu.com/p/56704058)

## 3 å¼•å…¥æ³¨æ„åŠ›æœºåˆ¶çš„Seq2seqæ¨¡å‹

æœ¬èŠ‚ä¸­å°†æ³¨æ„æœºåˆ¶æ·»åŠ åˆ°sequence to sequence æ¨¡å‹ä¸­ï¼Œä»¥æ˜¾å¼åœ°ä½¿ç”¨æƒé‡èšåˆstatesã€‚ä¸‹å›¾å±•ç¤ºencoding å’Œdecodingçš„æ¨¡å‹ç»“æ„ï¼Œåœ¨æ—¶é—´æ­¥ä¸ºtçš„æ—¶å€™ã€‚æ­¤åˆ»attention layerä¿å­˜ç€encoderingçœ‹åˆ°çš„æ‰€æœ‰ä¿¡æ¯â€”â€”å³encodingçš„æ¯ä¸€æ­¥è¾“å‡ºã€‚åœ¨decodingé˜¶æ®µï¼Œè§£ç å™¨çš„$t$æ—¶åˆ»çš„éšè—çŠ¶æ€è¢«å½“ä½œqueryï¼Œencoderçš„æ¯ä¸ªæ—¶é—´æ­¥çš„hidden statesä½œä¸ºkeyå’Œvalueè¿›è¡Œattentionèšåˆ. Attetion modelçš„è¾“å‡ºå½“ä½œæˆä¸Šä¸‹æ–‡ä¿¡æ¯context vectorï¼Œå¹¶ä¸è§£ç å™¨è¾“å…¥$D_t$æ‹¼æ¥èµ·æ¥ä¸€èµ·é€åˆ°è§£ç å™¨ï¼š

![Image Name](https://cdn.kesci.com/upload/image/q5km7o8z93.PNG?imageView2/0/w/800/h/800)

$$
Fig1å…·æœ‰æ³¨æ„æœºåˆ¶çš„seq-to-seqæ¨¡å‹è§£ç çš„ç¬¬äºŒæ­¥
$$


ä¸‹å›¾å±•ç¤ºäº†seq2seqæœºåˆ¶çš„æ‰€ä»¥å±‚çš„å…³ç³»ï¼Œä¸‹é¢å±•ç¤ºäº†encoderå’Œdecoderçš„layerç»“æ„

![Image Name](https://cdn.kesci.com/upload/image/q5km8dihlr.PNG?imageView2/0/w/800/h/800)

$$
Fig2å…·æœ‰æ³¨æ„æœºåˆ¶çš„seq-to-seqæ¨¡å‹ä¸­çš„å±‚ç»“æ„
$$

### è§£ç å™¨

   ç”±äºå¸¦æœ‰æ³¨æ„æœºåˆ¶çš„seq2seqçš„ç¼–ç å™¨ä¸ä¹‹å‰ç« èŠ‚ä¸­çš„Seq2SeqEncoderç›¸åŒï¼Œæ‰€ä»¥åœ¨æ­¤å¤„æˆ‘ä»¬åªå…³æ³¨è§£ç å™¨ã€‚æˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªMLPæ³¨æ„å±‚(MLPAttention)ï¼Œå®ƒçš„éšè—å¤§å°ä¸è§£ç å™¨ä¸­çš„LSTMå±‚ç›¸åŒã€‚ç„¶åæˆ‘ä»¬é€šè¿‡ä»ç¼–ç å™¨ä¼ é€’ä¸‰ä¸ªå‚æ•°æ¥åˆå§‹åŒ–è§£ç å™¨çš„çŠ¶æ€:

- the encoder outputs of all timestepsï¼šencoderè¾“å‡ºçš„å„ä¸ªçŠ¶æ€ï¼Œè¢«ç”¨äºattetion layerçš„memoryéƒ¨åˆ†ï¼Œæœ‰ç›¸åŒçš„keyå’Œvalues


- the hidden state of the encoderâ€™s final timestepï¼šç¼–ç å™¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œè¢«ç”¨äºåˆå§‹åŒ–decoder çš„hidden state


- the encoder valid length: ç¼–ç å™¨çš„æœ‰æ•ˆé•¿åº¦ï¼Œå€Ÿæ­¤ï¼Œæ³¨æ„å±‚ä¸ä¼šè€ƒè™‘ç¼–ç å™¨è¾“å‡ºä¸­çš„å¡«å……æ ‡è®°ï¼ˆPaddingsï¼‰


   åœ¨è§£ç çš„æ¯ä¸ªæ—¶é—´æ­¥ï¼Œæˆ‘ä»¬ä½¿ç”¨è§£ç å™¨çš„æœ€åä¸€ä¸ªRNNå±‚çš„è¾“å‡ºä½œä¸ºæ³¨æ„å±‚çš„queryã€‚ç„¶åï¼Œå°†æ³¨æ„åŠ›æ¨¡å‹çš„è¾“å‡ºä¸è¾“å…¥åµŒå…¥å‘é‡è¿æ¥èµ·æ¥ï¼Œè¾“å…¥åˆ°RNNå±‚ã€‚è™½ç„¶RNNå±‚éšè—çŠ¶æ€ä¹ŸåŒ…å«æ¥è‡ªè§£ç å™¨çš„å†å²ä¿¡æ¯ï¼Œä½†æ˜¯attention modelçš„è¾“å‡ºæ˜¾å¼åœ°é€‰æ‹©äº†enc_valid_lenä»¥å†…çš„ç¼–ç å™¨è¾“å‡ºï¼Œè¿™æ ·attentionæœºåˆ¶å°±ä¼šå°½å¯èƒ½æ’é™¤å…¶ä»–ä¸ç›¸å…³çš„ä¿¡æ¯ã€‚

```python
class Seq2SeqAttentionDecoder(d2l.Decoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqAttentionDecoder, self).__init__(**kwargs)
        self.attention_cell = MLPAttention(num_hiddens, num_hiddens, dropout)
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size + num_hiddens, num_hiddens, num_layers,
                           dropout=dropout)
        self.dense = nn.Linear(num_hiddens, vocab_size)

    def init_state(self, enc_outputs, enc_valid_len, *args):
        outputs, hidden_state = enc_outputs
        #         print("first:",outputs.size(),hidden_state[0].size(),hidden_state[1].size())
        # Transpose outputs to (batch_size, seq_len, hidden_size)
        return (outputs.permute(1, 0, -1), hidden_state, enc_valid_len)
        # outputs.swapaxes(0, 1)

    def forward(self, X, state):
        enc_outputs, hidden_state, enc_valid_len = state
        # ("X.size",X.size())
        X = self.embedding(X).transpose(0, 1)
        #         print("Xembeding.size2",X.size())
        outputs = []
        for l, x in enumerate(X):
            #             print(f"\n{l}-th token")
            #             print("x.first.size()",x.size())
            # query shape: (batch_size, 1, hidden_size)
            # select hidden state of the last rnn layer as query
            query = hidden_state[0][-1].unsqueeze(
                1)  # np.expand_dims(hidden_state[0][-1], axis=1)
            # context has same shape as query
            #             print("query enc_outputs, enc_outputs:\n",query.size(), enc_outputs.size(), enc_outputs.size())
            context = self.attention_cell(query, enc_outputs, enc_outputs,
                                          enc_valid_len)
            # Concatenate on the feature dimension
            #             print("context.size:",context.size())
            x = torch.cat((context, x.unsqueeze(1)), dim=-1)
            # Reshape x to (1, batch_size, embed_size+hidden_size)
            #             print("rnn",x.size(), len(hidden_state))
            out, hidden_state = self.rnn(x.transpose(0, 1), hidden_state)
            outputs.append(out)
        outputs = self.dense(torch.cat(outputs, dim=0))
        return outputs.transpose(0, 1), [enc_outputs, hidden_state,
                                         enc_valid_len]

```

ç°åœ¨æˆ‘ä»¬å¯ä»¥ç”¨æ³¨æ„åŠ›æ¨¡å‹æ¥æµ‹è¯•seq2seqã€‚ä¸ºäº†ä¸ç¬¬9.7èŠ‚ä¸­çš„æ¨¡å‹ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬å¯¹vocab_sizeã€embed_sizeã€num_hiddenså’Œnum_layersä½¿ç”¨ç›¸åŒçš„è¶…å‚æ•°ã€‚ç»“æœï¼Œæˆ‘ä»¬å¾—åˆ°äº†ç›¸åŒçš„è§£ç å™¨è¾“å‡ºå½¢çŠ¶ï¼Œä½†æ˜¯çŠ¶æ€ç»“æ„æ”¹å˜äº†ã€‚

```python
class Seq2SeqEncoder(d2l.Encoder):
    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers,
                 dropout=0, **kwargs):
        super(Seq2SeqEncoder, self).__init__(**kwargs)
        self.num_hiddens = num_hiddens
        self.num_layers = num_layers
        self.embedding = nn.Embedding(vocab_size, embed_size)
        self.rnn = nn.LSTM(embed_size, num_hiddens, num_layers,
                           dropout=dropout)

    def begin_state(self, batch_size, device):
        return [
            torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),
                        device=device),
            torch.zeros(size=(self.num_layers, batch_size, self.num_hiddens),
                        device=device)]

    def forward(self, X, *args):
        X = self.embedding(X)  # X shape: (batch_size, seq_len, embed_size)
        X = X.transpose(0, 1)  # RNN needs first axes to be time
        # state = self.begin_state(X.shape[1], device=X.device)
        out, state = self.rnn(X)
        # The shape of out is (seq_len, batch_size, num_hiddens).
        # out: æ¯ä¸ªrnnå•å…ƒçš„è¾“å‡ºï¼›æ˜¯ä¸€ä¸ªåºåˆ—
        # state contains the hidden state and the memory cell of the last
        # time step, the shape is (num_layers, batch_size, num_hiddens)
        return out, state


encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8,
                            num_hiddens=16, num_layers=2)
# encoder.initialize()
decoder = Seq2SeqAttentionDecoder(vocab_size=10, embed_size=8,
                                  num_hiddens=16, num_layers=2)
X = torch.zeros((4, 7),dtype=torch.long)
print("batch size=4\nseq_length=7\nhidden dim=16\nnum_layers=2\n")
print('encoder output size:', encoder(X)[0].size())
print('encoder hidden size:', encoder(X)[1][0].size())
print('encoder memory size:', encoder(X)[1][1].size())
state = decoder.init_state(encoder(X), None)
out, state = decoder(X, state)
out.shape, len(state), state[0].shape, len(state[1]), state[1][0].shape

```

```
batch size=4
seq_length=7
hidden dim=16
num_layers=2

encoder output size: torch.Size([7, 4, 16])
encoder hidden size: torch.Size([2, 4, 16])
encoder memory size: torch.Size([2, 4, 16])

(torch.Size([4, 7, 10]), 3, torch.Size([4, 7, 16]), 2, torch.Size([2, 4, 16]))
```

### è®­ç»ƒä¸é¢„æµ‹

ä»ç»“æœä¸­æˆ‘ä»¬å¯ä»¥çœ‹å‡ºï¼Œç”±äºè®­ç»ƒæ•°æ®é›†ä¸­çš„åºåˆ—ç›¸å¯¹è¾ƒçŸ­ï¼Œé¢å¤–çš„æ³¨æ„å±‚å¹¶æ²¡æœ‰å¸¦æ¥æ˜¾è‘—çš„æ”¹è¿›ã€‚ç”±äºç¼–ç å™¨å’Œè§£ç å™¨çš„æ³¨æ„å±‚çš„è®¡ç®—å¼€é”€ï¼Œè¯¥æ¨¡å‹æ¯”æ²¡æœ‰æ³¨æ„çš„seq2seqæ¨¡å‹æ…¢å¾—å¤šã€‚

```python
embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.0
batch_size, max_len = 64, 10
lr, num_epochs, ctx = 0.005, 500, d2l.try_gpu()
path_txt = "/home/zy/my_git/practice/deep_learning/Dive_into_DL/" \
           "materials/Task04/fraeng6506/fra.txt"

src_vocab, tgt_vocab, train_iter = \
    d2l.load_data_nmt(path_txt, batch_size, max_len, 50000)
encoder = d2l.Seq2SeqEncoder(
    len(src_vocab), embed_size, num_hiddens, num_layers, dropout)
decoder = Seq2SeqAttentionDecoder(
    len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)
model = d2l.EncoderDecoder(encoder, decoder)


d2l.train_s2s_ch9(model, train_iter, lr, num_epochs, ctx)


for sentence in ['Go .', 'Good Night !', "I'm OK .", 'I won !']:
    print(sentence + ' => ' + d2l.predict_s2s_ch9(
        model, sentence, src_vocab, tgt_vocab, max_len, ctx))

```

